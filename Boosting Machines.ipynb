{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboostNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading xgboost-2.0.3-py3-none-win_amd64.whl (99.8 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\prera\\anaconda3\\lib\\site-packages (from xgboost) (1.20.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\prera\\anaconda3\\lib\\site-packages (from xgboost) (1.6.2)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from mypipes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file=r'C:\\Users\\prera\\Downloads\\rg_train.csv'\n",
    "test_file=r'C:\\Users\\prera\\Downloads\\rg_test.csv'\n",
    "bd_train=pd.read_csv(train_file)\n",
    "\n",
    "bd_test=pd.read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars=list(bd_train.select_dtypes(exclude=['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars=[_ for _ in num_vars if _ not in ['REF_NO','Revenue.Grid']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars=list(bd_train.select_dtypes(include=['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars=[_ for _ in cat_vars if _ not in \n",
    "          ['children','age_band', 'post_code','post_area','family_income']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1=pdPipeline([\n",
    "    ('var_select',VarSelector(num_vars)),\n",
    "    ('missing_trt',DataFrameImputer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2=pdPipeline([\n",
    "    ('var_select',VarSelector(cat_vars)),\n",
    "    ('missing_trt',DataFrameImputer()),\n",
    "    ('create_dummies',get_dummies_Pipe(70))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3=pdPipeline([\n",
    "    ('var_select',VarSelector(['age_band'])),\n",
    "    ('custom_fico',custom_age_band()),\n",
    "    ('missing_trt',DataFrameImputer())\n",
    "])\n",
    "\n",
    "p4=pdPipeline([\n",
    "    ('var_select',VarSelector(['family_income'])),\n",
    "    ('custom_fico',custom_family_income()),\n",
    "    ('missing_trt',DataFrameImputer())\n",
    "])\n",
    "\n",
    "p5=pdPipeline([\n",
    "    ('var_select',VarSelector(['children'])),\n",
    "    ('string_clean1',string_clean(replace_it='Zero',replace_with='0')),\n",
    "    ('string_clean2',string_clean(replace_it='4+',replace_with='4')),\n",
    "    ('convert_to_numeric',convert_to_numeric()),\n",
    "    ('missing_trt',DataFrameImputer())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipe=FeatureUnion([\n",
    "    ('num',p1),\n",
    "    ('obj_to_dum',p2),\n",
    "    ('age_band',p3),\n",
    "    ('family_income',p4),\n",
    "    ('children',p5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=pd.DataFrame(data=data_pipe.fit_transform(bd_train),\n",
    "                     columns=data_pipe.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=pd.DataFrame(data=data_pipe.transform(bd_test),\n",
    "                     columns=data_pipe.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=(bd_train['Revenue.Grid']==1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 71)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_params={'n_estimators':[50,100,200,500,700],\n",
    "           'learning_rate': [0.01,.05,0.1,0.4,0.8,1],\n",
    "            'max_depth':[1,2,3,4,5,6],\n",
    "#             'min_samples_split':[2,5,10,20],\n",
    "#             'min_samples_leaf':[2,5,10,20],\n",
    "            'subsample':[0.5,0.8,1],\n",
    "            'max_features':[5,10,15,20,30,45,55,65]\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm=GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search=RandomizedSearchCV(gbm,\n",
    "                                 scoring='roc_auc',\n",
    "                                 param_distributions=gbm_params,\n",
    "                                 cv=10,\n",
    "                                 n_iter=10,\n",
    "                                 n_jobs=-1,\n",
    "                                verbose=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.05, loss='deviance', max_depth=5,\n",
    "              max_features=20, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=1, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=700,\n",
    "              n_iter_no_change=None, presort='auto', random_state=None,\n",
    "              subsample=0.5, tol=0.0001, validation_fraction=0.1,\n",
    "              verbose=0, warm_start=False)\n",
    "              \n",
    "use the above result in the class, its a result from previous run. This can be definitely different on a rerun. use this to save time in class so that you dont have to wait for the randomised search to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.5f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(random_search.cv_results_,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top 5 classfiers from the previous run were as follows : \n",
    "\n",
    "Model with rank: 1\n",
    "\n",
    "Mean validation score: 0.925 (std: 0.00188)\n",
    "\n",
    "Parameters: {'max_features': 20, 'max_depth': 3, 'subsample': 0.8, 'learning_rate': 0.4, 'n_estimators': 100}\n",
    "\n",
    "~~~~~~~~~~\n",
    "\n",
    "Model with rank: 2\n",
    "\n",
    "Mean validation score: 0.924 (std: 0.00121)\n",
    "\n",
    "Parameters: {'max_features': 15, 'max_depth': 4, 'subsample': 1, 'learning_rate': 0.4, 'n_estimators': 100}\n",
    "\n",
    "~~~~~~~~~~\n",
    "\n",
    "Model with rank: 3\n",
    "\n",
    "Mean validation score: 0.923 (std: 0.00250)\n",
    "\n",
    "Parameters: {'max_features': 5, 'max_depth': 4, 'subsample': 0.5, 'learning_rate': 0.05, 'n_estimators': 500}\n",
    "\n",
    "~~~~~~~~~~\n",
    "\n",
    "Model with rank: 4\n",
    "\n",
    "Mean validation score: 0.914 (std: 0.00290)\n",
    "\n",
    "Parameters: {'max_features': 10, 'max_depth': 5, 'subsample': 1, 'learning_rate': 0.05, 'n_estimators': 50}\n",
    "\n",
    "~~~~~~~~~~\n",
    "\n",
    "Model with rank: 5\n",
    "\n",
    "Mean validation score: 0.913 (std: 0.00174)\n",
    "\n",
    "Parameters: {'max_features': 30, 'max_depth': 5, 'subsample': 0.8, 'learning_rate': 0.4, 'n_estimators': 200}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tentative performance : 0.925 for the best classfier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: you can use the random search predict,predict_proba function to make prediction as randomisedsearchcv automatically fits the best candidate on complete data. If you want to look into feature_importance etc, then fit the best estimator separately**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {  \n",
    "                \"learning_rate\":[0.01,0.05,0.1,0.3,0.5],\n",
    "                \"gamma\":[i/10.0 for i in range(0,5)],\n",
    "                \"max_depth\": [2,3,4,5,6,7,8],\n",
    "                \"min_child_weight\":[1,2,5,10],\n",
    "                \"max_delta_step\":[0,1,2,5,10],\n",
    "                \"subsample\":[i/10.0 for i in range(5,10)],\n",
    "                \"colsample_bytree\":[i/10.0 for i in range(5,10)],\n",
    "                \"colsample_bylevel\":[i/10.0 for i in range(5,10)],\n",
    "                \"reg_lambda\":[1e-5, 1e-2, 0.1, 1, 100], \n",
    "                \"reg_alpha\":[1e-5, 1e-2, 0.1, 1, 100],\n",
    "                \"scale_pos_weight\":[1,2,3,4,5,6,7,8,9],\n",
    "                \"n_estimators\":[100,500,700,1000]\n",
    "             }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5*5*7*4*5*5*5*5*5*5*9*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb=XGBClassifier(objective='binary:logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter=10\n",
    "\n",
    "random_search=RandomizedSearchCV(xgb,n_jobs=-1,cv=10,n_iter=n_iter,scoring='roc_auc',\n",
    "                                 param_distributions=xgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(random_search.cv_results_,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top 5 classfiers from the previous run \n",
    "\n",
    "Model with rank: 1\n",
    "\n",
    "Mean validation score: 0.928 (std: 0.00232)\n",
    "\n",
    "Parameters: {'reg_lambda': 1e-05, 'subsample': 0.9, 'reg_alpha': 1, 'max_depth': 8, 'min_child_weight': 10, 'n_estimators': 1000, 'gamma': 0.1, 'colsample_bylevel': 0.8, 'scale_pos_weight': 2, 'colsample_bytree': 0.5, 'learning_rate': 0.01, 'max_delta_step': 10}\n",
    "\n",
    "____\n",
    "\n",
    "Model with rank: 2\n",
    "\n",
    "Mean validation score: 0.927 (std: 0.00160)\n",
    "\n",
    "Parameters: {'reg_lambda': 1, 'subsample': 0.6, 'reg_alpha': 0.1, 'max_depth': 4, 'min_child_weight': 2, 'n_estimators': 1000, 'gamma': 0.3, 'colsample_bylevel': 0.9, 'scale_pos_weight': 2, 'colsample_bytree': 0.7, 'learning_rate': 0.05, 'max_delta_step': 5}\n",
    "\n",
    "____\n",
    "\n",
    "Model with rank: 3\n",
    "\n",
    "Mean validation score: 0.926 (std: 0.00101)\n",
    "\n",
    "Parameters: {'reg_lambda': 0.1, 'subsample': 0.7, 'reg_alpha': 1e-05, 'max_depth': 5, 'min_child_weight': 5, 'n_estimators': 500, 'gamma': 0.2, 'colsample_bylevel': 0.5, 'scale_pos_weight': 3, 'colsample_bytree': 0.9, 'learning_rate': 0.1, 'max_delta_step': 1}\n",
    "\n",
    "____\n",
    "\n",
    "Model with rank: 4\n",
    "\n",
    "Mean validation score: 0.925 (std: 0.00104)\n",
    "\n",
    "Parameters: {'reg_lambda': 0.1, 'subsample': 0.9, 'reg_alpha': 0.01, 'max_depth': 6, 'min_child_weight': 2, 'n_estimators': 1000, 'gamma': 0.2, 'colsample_bylevel': 0.8, 'scale_pos_weight': 8, 'colsample_bytree': 0.5, 'learning_rate': 0.05, 'max_delta_step': 0}\n",
    "\n",
    "____\n",
    "\n",
    "Model with rank: 5\n",
    "\n",
    "Mean validation score: 0.920 (std: 0.00278)\n",
    "\n",
    "Parameters: {'reg_lambda': 1, 'subsample': 0.8, 'reg_alpha': 0.1, 'max_depth': 5, 'min_child_weight': 5, 'n_estimators': 500, 'gamma': 0.0, 'colsample_bylevel': 0.5, 'scale_pos_weight': 8, 'colsample_bytree': 0.5, 'learning_rate': 0.01, 'max_delta_step': 5}\n",
    "\n",
    "____\n",
    "\n",
    "tentative performance of best estimator : 0.928\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best estimator from the previous run can be copied from here :\n",
    "\n",
    "XGBClassifier(base_score=0.5, colsample_bylevel=0.8, colsample_bytree=0.5,\n",
    "       gamma=0.1, learning_rate=0.01, max_delta_step=10, max_depth=8,\n",
    "       min_child_weight=10, missing=None, n_estimators=1000, nthread=-1,\n",
    "       objective='binary:logistic', reg_alpha=1, reg_lambda=1e-05,\n",
    "       scale_pos_weight=2, seed=0, silent=True, subsample=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: you can use the random search predict,predict_proba function to make prediction as randomisedsearchcv automatically fits the best candidate on complete data. If you want to look into feature_importance etc, then fit the best estimator separately**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Parameter tuning for xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we tune all the paramters together , there are chances that our results will be much far from the best. There are many parameters where variation doesnt impact the performance too much and we can tune them later once we have fixed values of parameters with volatile performance.\n",
    "\n",
    "As a general strtaegy you can start with tuning numer of trees or n_estimators , in case of boosting machines , learning_rate is directly related with n_estimators . A very low learning_rate will need high number of n_estimators . We can start with a decent fixed learning rate and tune n_estimaors for it. \n",
    "\n",
    "All can be left as default for now except subsample , colsample_bytree and colsample_bylevel, these are set to default=1, we'll take a more conservative value 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {  \n",
    "                \"n_estimators\":[100,500,700,900,1000,1200,1500]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1=XGBClassifier(subsample=0.8,\n",
    "                   colsample_bylevel=0.8,\n",
    "                   colsample_bytree=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search=GridSearchCV(xgb1,cv=10,\n",
    "                         param_grid=xgb_params,\n",
    "                         scoring='roc_auc',\n",
    "                         verbose=20)\n",
    "\n",
    "# two issues : currently xgboost is not running with multicores \n",
    "# mac issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(grid_search.cv_results_,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we got n_estimator=500 as best with learning_rate=0.1  . Next we'll tune max_depth,gamma and min_child_weight, which control overfit by controlling size of individual trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params={\n",
    "            \"gamma\":[0,2,5,8,10],\n",
    "            \"max_depth\": [2,3,4,5,6,7,8],\n",
    "            \"min_child_weight\":[0.5,1,2,5,10]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb2=XGBClassifier(learning_rate=0.1,n_estimators=500,\n",
    "                   subsample=0.8,colsample_bylevel=0.8,colsample_bytree=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search=RandomizedSearchCV(xgb2,\n",
    "                                 param_distributions=xgb_params,n_iter=20,cv=5,scoring='roc_auc',\n",
    "                                 n_jobs=-1,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(random_search.cv_results_,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we got best values for parameters being tuned as follows : {'min_child_weight': 1, 'gamma': 0, 'max_depth': 3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is imbalance in the data , we'll look into max_delta_step and scale_pos_weight next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24720/7841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params={\n",
    "            'max_delta_step':[0,1,3,6,10],\n",
    "            'scale_pos_weight':[1,2,3,4]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb3=XGBClassifier(learning_rate=0.1,\n",
    "                   n_estimators=500,min_child_weight=1,\n",
    "                   gamma=0,max_depth=3,\n",
    "                   \n",
    "                  subsample=0.8,colsample_bylevel=0.8,colsample_bytree=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search=GridSearchCV(xgb3,param_grid=xgb_params,cv=5,scoring='roc_auc',n_jobs=-1,\n",
    "                         verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(grid_search.cv_results_,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it turns out that , since imbalance was not that severe , defaults come out as best choices {'scale_pos_weight': 1, 'max_delta_step': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we check the effect of the noise on data and tune , subsample , colsample_bytree and colsample_bylevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params={\n",
    "            'subsample':[i/10 for i in range(5,11)],\n",
    "            'colsample_bytree':[i/10 for i in range(5,11)],\n",
    "            'colsample_bylevel':[i/10 for i in range(5,11)]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb4=XGBClassifier(learning_rate=0.1,n_estimators=500,min_child_weight=1,gamma=0,max_depth=3,\n",
    "                        scale_pos_weight=1,max_delta_step=0\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search=RandomizedSearchCV(xgb4,param_distributions=xgb_params,cv=5,n_iter=20,scoring='roc_auc',\n",
    "                                n_jobs=-1,verbose=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(random_search.cv_results_,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bets values that we got for paramaeters are as follows : {'colsample_bylevel': 0.5, 'colsample_bytree': 0.6, 'subsample': 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lastly we can work on L2 and L1 penalty on leaf node score to further reduce overfit if there is any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb5=XGBClassifier(learning_rate=0.1,n_estimators=500,min_child_weight=1,gamma=0,max_depth=3,\n",
    "                        scale_pos_weight=1,max_delta_step=0,\n",
    "                   colsample_bylevel= 0.5, colsample_bytree= 0.6, subsample= 1.0\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params={\n",
    "            'reg_lambda':[i/10 for i in range(0,50)],\n",
    "            'reg_alpha':[i/10 for i in range(0,50)]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search=RandomizedSearchCV(xgb5,param_distributions=xgb_params,cv=5,n_iter=20,scoring='roc_auc',\n",
    "                                n_jobs=-1,verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report(random_search.cv_results_,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best value that we got here is {'reg_lambda': 1.5, 'reg_alpha': 0.0}, but the performance has gone down. May be the default was doing better and wasnt picked as one of the candidates here in the random_search. we'll go with those defaults values instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb6=XGBClassifier(learning_rate=0.1,n_estimators=500,min_child_weight=1,gamma=0,max_depth=3,\n",
    "                        scale_pos_weight=1,max_delta_step=0,\n",
    "                   colsample_bylevel= 0.5, colsample_bytree= 0.6, subsample= 1.0,\n",
    "                  reg_lambda=1,reg_alpha=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to simply get cv performance of a model , without having to select any parameters we can make use of cross_validation_score function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(xgb6,x_train,y_train,scoring='roc_auc',verbose=10,n_jobs=-1,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[0.92951477, 0.92590096, 0.93070889, 0.92176974, 0.92882013,\n",
    "       0.93128318, 0.93018259, 0.93297173, 0.93256565, 0.92947388]\n",
    "# these are from an earlier iteration , need not match with your current run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
